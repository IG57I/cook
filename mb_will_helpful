---------------------обшая настройка----------------------------------
#https://russianblogs.com/article/40391171020/ ------add in jupiter

#view pandas
# Сброс ограничений на количество выводимых рядов
pd.set_option('display.max_rows', None)
# Сброс ограничений на число столбцов
pd.set_option('display.max_columns', None)
# Сброс ограничений на количество символов в записи
pd.set_option('display.max_colwidth', None)

#import
import time
from tqdm import tqdm
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pymorphy2
import re
import numpy as np
import scipy.stats as ss
from sklearn.feature_extraction.text import CountVectorizer
import wordcloud
from wordcloud import WordCloud

#importing plotly and cufflinks in offline mode
import cufflinks as cf
import plotly.offline
cf.go_offline()
cf.set_config_file(offline=False, world_readable=True)


import plotly 
import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as py
from plotly.offline import iplot
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

import warnings
warnings.filterwarnings("ignore")

#missing
def missing (df):
    missing_number = df.isnull().sum().sort_values(ascending=False)
    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return missing_values

missing(df)
---------------------обшая настройка----------------------------------

---------------------Визуализация данных----------------------------------
# для кат калонок
Что нам нужно, это то, что будетсмотретькак корреляция, но будет работать с категориальными значениями - или, более формально, мы ищеммера ассоциациимежду двумя категориальными чертами. Вводя:Крамер V, Он основан на номинальном измененииКритерий хи-квадрат Пирсонаи поставляется со встроенными преимуществами:
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x,y)
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
    rcorr = r-((r-1)**2)/(n-1)
    kcorr = k-((k-1)**2)/(n-1)
    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))

#график для подсчета
sns.barplot(data[''].value_counts().loc[lambda x : x > 3].index,data[''].value_counts().loc[lambda x : x > 3].values)
plt.xticks(rotation=90);

#визуализация wordcloud для мешков слов
cv = CountVectorizer()
cvA = cv.fit_transform(OTDF["W"]).toarray()
cvDF = pd.DataFrame(cvA, columns=cv.get_feature_names())

for c in tqdm(range(len(cvDF.groupby(['T']).sum()))):
    w=[]
    for i,j in zip(cvDF.groupby(['T']).sum().T.index, cvDF.groupby(['T']).sum().T.iloc[:,c].values):
        a=[]
        if j >= 1:
            a.append(i)
            a.append(j)
            w.append(a)
    pd.DataFrame(w)
    data = dict(w)
    wc = WordCloud(width=800, height=400, max_words=200).generate_from_frequencies(data)
    plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(cvDF.groupby(['T']).sum().T.columns[c])
    plt.show()
    time.sleep(1)
#  heatmap можно посмотреть частоту коррелирующих признаков
cv = CountVectorizer(min_df=0.02, max_df=0.5)
cvA = cv.fit_transform(data).toarray()
cvDF = pd.DataFrame(cvA, columns=cv.get_feature_names())
corr = cvDF.corr()
plt.figure(figsize=(15,10))
sns.heatmap(corr)
---------------------Визуализация данных----------------------------------

---------------------Работа с текстом----------------------------------
# чистка текста def pretext(column,data):
  data[column] = data[column].apply(lambda x: re.sub("[^a-zA-Zа-яА-Я]", " ", x).lower().strip()) # удоляем символы 
  data[column] = data[column].apply(lambda x: " ".join(word for word in x.split() if word not in stopwords))# удоляем стоп слова
  morph = pymorphy2.MorphAnalyzer()
  data[column] = data[column].apply(lambda x: " ".join(morph.parse(word)[0].normal_form for word in x.split())) # проводим Лемматизацию
  
 #стоп слова
stopwords = ["должет","знать","понимать","специалист","c","а","алло","без","белый","близко","более","больше","большой","будем","будет","будете","будешь","будто","буду","будут","будь","бы","бывает","бывь","был","была","были","было","быть","в","важная","важное","важные","важный","вам","вами","вас","ваш","ваша","ваше","ваши","вверх","вдали","вдруг","ведь","везде","вернуться","весь","вечер","взгляд","взять","вид","видел","видеть","вместе","вне","вниз","внизу","во","вода","война","вокруг","вон","вообще","вопрос","восемнадцатый","восемнадцать","восемь","восьмой","вот","впрочем","времени","время","все","все еще","всегда","всего","всем","всеми","всему","всех","всею","всю","всюду","вся","всё","второй","вы","выйти","г","где","главный","глаз","говорил","говорит","говорить","год","года","году","голова","голос","город","да","давать","давно","даже","далекий","далеко","дальше","даром","дать","два","двадцатый","двадцать","две","двенадцатый","двенадцать","дверь","двух","девятнадцатый","девятнадцать","девятый","девять","действительно","дел","делал","делать","делаю","дело","день","деньги","десятый","десять","для","до","довольно","долго","должен","должно","должный","дом","дорога","друг","другая","другие","других","друго","другое","другой","думать","душа","е","его","ее","ей","ему","если","есть","еще","ещё","ею","её","ж","ждать","же","жена","женщина","жизнь","жить","за","занят","занята","занято","заняты","затем","зато","зачем","здесь","земля","знать","значит","значить","и","иди","идти","из","или","им","имеет","имел","именно","иметь","ими","имя","иногда","их","к","каждая","каждое","каждые","каждый","кажется","казаться","как","какая","какой","кем","книга","когда","кого","ком","комната","кому","конец","конечно","которая","которого","которой","которые","который","которых","кроме","кругом","кто","куда","лежать","лет","ли","лицо","лишь","лучше","любить","люди","м","маленький","мало","мать","машина","между","меля","менее","меньше","меня","место","миллионов","мимо","минута","мир","мира","мне","много","многочисленная","многочисленное","многочисленные","многочисленный","мной","мною","мог","могу","могут","мож","может","может быть","можно","можхо","мои","мой","мор","москва","мочь","моя","моё","мы","на","наверху","над","надо","назад","наиболее","найти","наконец","нам","нами","народ","нас","начала","начать","наш","наша","наше","наши","не","него","недавно","недалеко","нее","ней","некоторый","нельзя","нем","немного","нему","непрерывно","нередко","несколько","нет","нею","неё","ни","нибудь","ниже","низко","никакой","никогда","никто","никуда","ним","ними","них","ничего","ничто","но","новый","нога","ночь","ну","нужно","нужный","нх","о","об","оба","обычно","один","одиннадцатый","одиннадцать","однажды","однако","одного","одной","оказаться","окно","около","он","она","они","оно","опять","особенно","остаться","от","ответить","отец","откуда","отовсюду","отсюда","очень","первый","перед","писать","плечо","по","под","подойди","подумать","пожалуйста","позже","пойти","пока","пол","получить","помнить","понимать","понять","пор","пора","после","последний","посмотреть","посреди","потом","потому","почему","почти","правда","прекрасно","при","про","просто","против","процентов","путь","пятнадцатый","пятнадцать","пятый","пять","работа","работать","раз","разве","рано","раньше","ребенок","решить","россия","рука","русский","ряд","рядом","с","с кем","сам","сама","сами","самим","самими","самих","само","самого","самой","самом","самому","саму","самый","свет","свое","своего","своей","свои","своих","свой","свою","сделать","сеаой","себе","себя","сегодня","седьмой","сейчас","семнадцатый","семнадцать","семь","сидеть","сила","сих","сказал","сказала","сказать","сколько","слишком","слово","случай","смотреть","сначала","снова","со","собой","собою","советский","совсем","спасибо","спросить","сразу","стал","старый","стать","стол","сторона","стоять","страна","суть","считать","т","та","так","такая","также","таки","такие","такое","такой","там","твои","твой","твоя","твоё","те","тебе","тебя","тем","теми","теперь","тех","то","тобой","тобою","товарищ","тогда","того","тоже","только","том","тому","тот","тою","третий","три","тринадцатый","тринадцать","ту","туда","тут","ты","тысяч","у","увидеть","уж","уже","улица","уметь","утро","хороший","хорошо","хотел бы","хотеть","хоть","хотя","хочешь","час","часто","часть","чаще","чего","человек","чем","чему","через","четвертый","четыре","четырнадцатый","четырнадцать","что","чтоб","чтобы","чуть","шестнадцатый","шестнадцать","шестой","шесть","эта","эти","этим","этими","этих","это","этого","этой","этом","этому","этот","эту","я","являюсь"]

#Частотный словарь покажет частоту встречающихся слов в данных
cv = CountVectorizer()
cbA = cv.fit_transform(data1_2['Краткое\nсодержание']).toarray()
dfwf = pd.DataFrame(cv.vocabulary_ ,index=['Чaстота'])
dfwf
#Векторизированный словарь по суть посчитает кол-во слов в строке.
cv = CountVectorizer()
cvA = cv.fit_transform(data1_2['']).toarray()
cvDF = pd.DataFrame(cvA, columns=cv.get_feature_names())
cvDF["T"] = data[''].values
---------------------Работа с текстом----------------------------------

---------------------Обученние моделей----------------------------------
# Импорты
import time
from tqdm import tqdm
import pandas as pd 
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer
import numpy as np
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")
# models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.model_selection import GridSearchCV
import seaborn as sns 
# saving
import pickle


---------------------Обученние моделей----------------------------------
# Feature Engineering
data3_2['word_count'] = data3_2["X"].apply(lambda x: len(str(x).split(" "))) # подсчет кол-во слов
data3_2['char_count'] = data3_2["X"].apply(lambda x: sum(len(word) for word in str(x).split(" "))) # подсчет кол-во символов
data3_2['avg_word_length'] = data3_2['char_count'] / data3_2['word_count'] # средняя длинна слова
data3_2['unique_word_count'] = data3_2["X"].apply(lambda x:len(set(x.split()))) # количество уникальых слов
data3_2['unique_vs_words'] = data3_2['unique_word_count']/data3_2['word_count'] # частота уникальных слов
--------------------------------------------------------------------------------------------
https://dev-gang.ru/article/sozdaite-prostoi-czat-bot-s-python-i-google-search-wptggrso22/
https://www.kaggle.com/pompelmo/chaieda-police-violence/notebook
state_geo = ''
import json
with open(state_geo, encoding="utf8") as f:
    map_data = json.load(f)
    
import pandas as pd
from pandas.io.json import json_normalize
df = json_normalize(map_data["features"])

coords = 'geometry.coordinates'

df2 = (df[coords].apply(lambda r: [(i[0],i[1]) for i in r[0]])
           .apply(pd.Series).stack()
           .reset_index(level=1).rename(columns={0:coords,"level_1":"point"})
           .join(df.drop(coords,1), how='left')).reset_index(level=0)

df2[['lat','long']] = df2[coords].apply(pd.Series)

df2  


states_order = [state['id'] for state in map_data['features']]
for idx in range(len(states_order)):
        map_data['features'][idx]['properties']['int'] = int(data[data['id'] == states_order[idx]]['index'].values[0])

map_b = folium.Map(location=[37, -102], zoom_start=5)

chorop_map = folium.Choropleth(
    geo_data=map_data,
    name='choropleth',
    data=data,
    columns=['id','index'],
    key_on='feature.id',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Number of police kills every 1M ppl by state',
    highlight=True
).add_to(map_b)

folium.LayerControl().add_to(map_b)
chorop_map.geojson.add_child(
    folium.features.GeoJsonTooltip(['name','int'])
)
map_b
